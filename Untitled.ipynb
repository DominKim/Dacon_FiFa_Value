{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_train=pd.read_csv(\"../FIFA_train.csv\")\n",
    "fifa_test=pd.read_csv(\"../FIFA_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_train['contract_until']=fifa_train['contract_until'].str.slice(-4,).astype(float)\n",
    "fifa_test['contract_until']=fifa_test['contract_until'].str.slice(-4,).astype(float)\n",
    "\n",
    "fifa_train['prefer_foot']=fifa_train['prefer_foot'].map({'left':0,'right':1})\n",
    "fifa_test['prefer_foot']=fifa_test['prefer_foot'].map({'left':0,'right':1})\n",
    "\n",
    "fifa_train['position']=fifa_train['position'].map({'ST':0,'GK':1,'DF':2,'MF':4})\n",
    "fifa_test['position']=fifa_test['position'].map({'ST':0,'GK':1,'DF':2,'MF':4})\n",
    "\n",
    "fifa_train['continent']=fifa_train['continent'].map({'south america':0,\n",
    "                                                     'europe':1,\n",
    "                                                     'africa':2,\n",
    "                                                     'asia':3,\n",
    "                                                     'oceania':4})\n",
    "fifa_test['continent']=fifa_test['continent'].map({'south america':0,\n",
    "                                                     'europe':1,\n",
    "                                                     'africa':2,\n",
    "                                                     'asia':3,\n",
    "                                                     'oceania':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_train['total_rep_skill']=fifa_train['reputation']+fifa_train['stat_skill_moves']\n",
    "fifa_test['total_rep_skill']=fifa_test['reputation']+fifa_test['stat_skill_moves']\n",
    "\n",
    "fifa_train['diff_rep_skill']=fifa_train['reputation']-fifa_train['stat_skill_moves']\n",
    "fifa_test['diff_rep_skill']=fifa_test['reputation']-fifa_test['stat_skill_moves']\n",
    "\n",
    "fifa_train['total_stat']=fifa_train['stat_potential']+fifa_train['stat_overall']\n",
    "fifa_test['total_stat']=fifa_test['stat_potential']+fifa_test['stat_overall']\n",
    "\n",
    "fifa_train['diff_stat']=fifa_train['stat_potential']-fifa_train['stat_overall']\n",
    "fifa_test['diff_stat']=fifa_test['stat_potential']-fifa_test['stat_overall']\n",
    "\n",
    "fifa_train['total_stat']=np.log(fifa_train['total_stat'])\n",
    "fifa_test['total_stat']=np.log(fifa_test['total_stat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_train['continent_ratio']=[0]*len(fifa_train)\n",
    "\n",
    "for i,n in zip((fifa_train['continent'].value_counts()/len(fifa_train)).index,(fifa_train['continent'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_train.loc[fifa_train['continent']==i,'continent_ratio']=n\n",
    "    \n",
    "    \n",
    "fifa_train['position_ratio']=[0]*len(fifa_train)\n",
    "\n",
    "for i,n in zip((fifa_train['position'].value_counts()/len(fifa_train)).index,(fifa_train['position'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_train.loc[fifa_train['position']==i,'position_ratio']=n\n",
    "    \n",
    "\n",
    "fifa_train['foot_ratio']=[0]*len(fifa_train)\n",
    "\n",
    "for i,n in zip((fifa_train['prefer_foot'].value_counts()/len(fifa_train)).index,(fifa_train['prefer_foot'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_train.loc[fifa_train['prefer_foot']==i,'foot_ratio']=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_test['continent_ratio']=[0]*len(fifa_test)\n",
    "\n",
    "for i,n in  zip((fifa_train['continent'].value_counts()/len(fifa_train)).index,(fifa_train['continent'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_test.loc[fifa_test['continent']==i,'continent_ratio']=n\n",
    "    \n",
    "    \n",
    "fifa_test['position_ratio']=[0]*len(fifa_test)\n",
    "\n",
    "for i,n in zip((fifa_train['position'].value_counts()/len(fifa_train)).index,(fifa_train['position'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_test.loc[fifa_test['position']==i,'position_ratio']=n\n",
    "    \n",
    "\n",
    "fifa_test['foot_ratio']=[0]*len(fifa_test)\n",
    "\n",
    "for i,n in zip((fifa_train['prefer_foot'].value_counts()/len(fifa_train)).index,(fifa_train['prefer_foot'].value_counts()/len(fifa_train)).values):\n",
    "    fifa_test.loc[fifa_test['prefer_foot']==i,'foot_ratio']=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train=fifa_train.copy()\n",
    "new_test=fifa_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.log1p(new_train['value'])\n",
    "new_train.drop(['id','name','value'],axis=1,inplace=True)\n",
    "new_test.drop(['id','name'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "lgbm = LGBMRegressor(boosting_type=\"dart\",random_state = 0)\n",
    "lgbm.fit(new_train,target)\n",
    "\n",
    "prediction=np.expm1(lgbm.predict(new_test))\n",
    "\n",
    "sub_example=pd.read_csv(\"../submission.csv\")\n",
    "sub_example['value']=prediction\n",
    "sub_example.to_csv(\"../submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.log1p(new_train['value'])\n",
    "new_train.drop(['id','name','value'],axis=1,inplace=True)\n",
    "new_test.drop(['id','name'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_lgbm(params, train_data, test_data, target_data, num_round, early_round, verbose_round, N_SPLITS=5, random_state=0):\n",
    "\n",
    "    FOLDs=KFold(n_splits=N_SPLITS, shuffle=True,random_state=0)\n",
    "\n",
    "    oof = np.zeros(len(train_data))\n",
    "    predictions = np.zeros(len(test_data))\n",
    "\n",
    "    features_lgb = list(train_data.columns)\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_data)):\n",
    "        trn_data = lgb.Dataset(train_data.iloc[trn_idx], label=target_data.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_data.iloc[val_idx], label=target_data.iloc[val_idx])\n",
    "\n",
    "        print(\"LGB \" + str(fold_) + \"-\" * 50)\n",
    "        num_round = num_round\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose_round, \n",
    "                        early_stopping_rounds = early_round)\n",
    "        oof[val_idx] = clf.predict(train_data.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features_lgb\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += clf.predict(test_data, num_iteration=clf.best_iteration) / FOLDs.n_splits\n",
    "    return oof, predictions, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def get_oof_xgb(params, train_data, test_data, target_data, num_round, early_round, verbose_round, N_SPLITS=5, random_state=0):\n",
    "\n",
    "    FOLDs=KFold(n_splits=N_SPLITS, shuffle=True,random_state=0)\n",
    "\n",
    "    oof = np.zeros(len(train_data))\n",
    "    predictions = np.zeros(len(test_data))\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    best_iters = []\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_data)):\n",
    "        trn_data = xgb.DMatrix(train_data.iloc[trn_idx], label=target_data.iloc[trn_idx])\n",
    "        val_data = xgb.DMatrix(train_data.iloc[val_idx], label=target_data.iloc[val_idx])\n",
    "\n",
    "#         print(\"xgb \" + str(fold_) + \"-\" * 50)\n",
    "\n",
    "        watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "#         print(\"xgb \" + str(fold_) + \"-\" * 50)\n",
    "        num_round = num_round\n",
    "        xgb_model = xgb.train(params, trn_data, num_round, watchlist, \n",
    "                              early_stopping_rounds=early_round, verbose_eval=verbose_round)\n",
    "        oof[val_idx] = xgb_model.predict(xgb.DMatrix(train_data.iloc[val_idx]), \n",
    "                                             ntree_limit=xgb_model.best_ntree_limit)\n",
    "\n",
    "        predictions += xgb_model.predict(xgb.DMatrix(test_data), \n",
    "                                             ntree_limit=xgb_model.best_ntree_limit) / FOLDs.n_splits\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame([xgb_model.get_score()]).T.reset_index()\n",
    "        fold_importance_df.columns = ['feature', 'importance']\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        best_iters.append(xgb_model.best_ntree_limit)\n",
    "    return oof, predictions, feature_importance_df, np.mean(best_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:05:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:05:58] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { metric, min_child_samples, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:12.83666\tvalid-rmse:12.84015\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 400 rounds.\n",
      "[500]\ttrain-rmse:0.07003\tvalid-rmse:0.07695\n",
      "[1000]\ttrain-rmse:0.06572\tvalid-rmse:0.07374\n",
      "[1500]\ttrain-rmse:0.06369\tvalid-rmse:0.07227\n",
      "[2000]\ttrain-rmse:0.06229\tvalid-rmse:0.07161\n",
      "[2500]\ttrain-rmse:0.06136\tvalid-rmse:0.07132\n",
      "[3000]\ttrain-rmse:0.06051\tvalid-rmse:0.07057\n",
      "[3500]\ttrain-rmse:0.05998\tvalid-rmse:0.07046\n",
      "Stopping. Best iteration:\n",
      "[3289]\ttrain-rmse:0.06009\tvalid-rmse:0.07035\n",
      "\n",
      "[20:06:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { metric, min_child_samples, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:12.84263\tvalid-rmse:12.82023\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 400 rounds.\n",
      "[500]\ttrain-rmse:0.07111\tvalid-rmse:0.08571\n",
      "[1000]\ttrain-rmse:0.06701\tvalid-rmse:0.07994\n",
      "[1500]\ttrain-rmse:0.06473\tvalid-rmse:0.07756\n",
      "[2000]\ttrain-rmse:0.06343\tvalid-rmse:0.07748\n",
      "[2500]\ttrain-rmse:0.06269\tvalid-rmse:0.07676\n",
      "[3000]\ttrain-rmse:0.06174\tvalid-rmse:0.07605\n",
      "[3500]\ttrain-rmse:0.06102\tvalid-rmse:0.07533\n",
      "Stopping. Best iteration:\n",
      "[3542]\ttrain-rmse:0.06098\tvalid-rmse:0.07465\n",
      "\n",
      "[20:06:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:11] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { metric, min_child_samples, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:12.83251\tvalid-rmse:12.85732\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 400 rounds.\n",
      "[500]\ttrain-rmse:0.06978\tvalid-rmse:0.07427\n",
      "[1000]\ttrain-rmse:0.06578\tvalid-rmse:0.07215\n",
      "[1500]\ttrain-rmse:0.06370\tvalid-rmse:0.07092\n",
      "[2000]\ttrain-rmse:0.06238\tvalid-rmse:0.06859\n",
      "[2500]\ttrain-rmse:0.06151\tvalid-rmse:0.06842\n",
      "[3000]\ttrain-rmse:0.06084\tvalid-rmse:0.06728\n",
      "[3500]\ttrain-rmse:0.06022\tvalid-rmse:0.06671\n",
      "[4000]\ttrain-rmse:0.05964\tvalid-rmse:0.06636\n",
      "Stopping. Best iteration:\n",
      "[4060]\ttrain-rmse:0.05957\tvalid-rmse:0.06604\n",
      "\n",
      "[20:06:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { metric, min_child_samples, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:12.82802\tvalid-rmse:12.88117\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 400 rounds.\n",
      "[500]\ttrain-rmse:0.07213\tvalid-rmse:0.08119\n",
      "[1000]\ttrain-rmse:0.06757\tvalid-rmse:0.07784\n",
      "[1500]\ttrain-rmse:0.06536\tvalid-rmse:0.07661\n",
      "[2000]\ttrain-rmse:0.06384\tvalid-rmse:0.07570\n",
      "[2500]\ttrain-rmse:0.06282\tvalid-rmse:0.07501\n",
      "[3000]\ttrain-rmse:0.06223\tvalid-rmse:0.07481\n",
      "Stopping. Best iteration:\n",
      "[2713]\ttrain-rmse:0.06255\tvalid-rmse:0.07456\n",
      "\n",
      "[20:06:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:06:23] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { metric, min_child_samples, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:12.84827\tvalid-rmse:12.78957\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 400 rounds.\n",
      "[500]\ttrain-rmse:0.07070\tvalid-rmse:0.07358\n",
      "[1000]\ttrain-rmse:0.06611\tvalid-rmse:0.07020\n",
      "[1500]\ttrain-rmse:0.06408\tvalid-rmse:0.06894\n",
      "[2000]\ttrain-rmse:0.06281\tvalid-rmse:0.06792\n",
      "[2500]\ttrain-rmse:0.06198\tvalid-rmse:0.06682\n",
      "[3000]\ttrain-rmse:0.06156\tvalid-rmse:0.06688\n",
      "[3500]\ttrain-rmse:0.06101\tvalid-rmse:0.06651\n",
      "[4000]\ttrain-rmse:0.06060\tvalid-rmse:0.06594\n",
      "Stopping. Best iteration:\n",
      "[4033]\ttrain-rmse:0.06059\tvalid-rmse:0.06587\n",
      "\n",
      "[20:06:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "xgb_params={\"objective\":\"reg:linear\",\n",
    "           \"metric\":\"rmse\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"silent\":True}\n",
    "a,pre_xgb,feature_impo_xgb,d=get_oof_xgb(xgb_params, new_train, new_test, target, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.expm1(pre_xgb)\n",
    "\n",
    "sub_example=pd.read_csv(\"../submission.csv\")\n",
    "sub_example['value']=prediction\n",
    "sub_example.to_csv(\"../submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
